{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tempfile\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import subprocess\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "Import Parquet As a DataFrame\n",
    "\"\"\"\n",
    "\n",
    "##Read in parquet file from public S3 bucket\n",
    "parquet_s3 = \"s3://steichenetalpublicdata/analyzed_sequences/parquet\"\n",
    "df_spark = spark.read.parquet(parquet_s3)\n",
    "\n",
    "# allow pyspark to use apache arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a query class\n",
    "\n",
    "The query class can hold our spark query until it's time to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_dfs(list_of_dfs):\n",
    "    \"\"\"helper function to combine many queries back into one\"\"\"\n",
    "    combined_counts = (\n",
    "        pd.concat(list_of_dfs)\n",
    "        .groupby([\"ez_donor\"])\n",
    "        .sum()\n",
    "        .drop([\"total_count\", \"NormalizedCustomerValue\"], axis=1)\n",
    "    )\n",
    "    combined_sums = pd.concat(\n",
    "        [i.set_index([\"ez_donor\"])[\"total_count\"] for i in list_of_dfs]\n",
    "    )\n",
    "    combined_sums = (\n",
    "        combined_sums.reset_index().groupby([\"ez_donor\"]).head(1).set_index(\"ez_donor\")\n",
    "    )\n",
    "    combined = combined_counts.join(combined_sums)\n",
    "    combined[\"NormalizedCustomerValue\"] = combined[\"count\"] / combined[\"total_count\"]\n",
    "    return combined\n",
    "\n",
    "\n",
    "class Query:\n",
    "\n",
    "    \"\"\"An example query class to hold query parameters\"\"\"\n",
    "\n",
    "    def __init__(self, q_name, length=\"\", v_fam=\"\", d_gene=\"\", j_gene=\"\", regex=\"\"):\n",
    "        self.query_name = q_name\n",
    "        self.v_fam = v_fam\n",
    "        self.j_gene = j_gene\n",
    "        self.d_gene = d_gene\n",
    "\n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "        self.applied = False\n",
    "\n",
    "    def apply(self, df):\n",
    "\n",
    "        \"\"\"Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "\n",
    "        Returns a filtered dataframe\n",
    "        \"\"\"\n",
    "        self.queried_dataframe = \"\"\n",
    "\n",
    "        ##Lets get length\n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) == self.length)\n",
    "\n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "        if self.v_fam:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(\n",
    "                self.queried_dataframe.v_fam == self.v_fam\n",
    "            )\n",
    "\n",
    "        if self.d_gene:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(\n",
    "                self.queried_dataframe.d_gene == self.d_gene\n",
    "            )\n",
    "\n",
    "        if self.j_gene:\n",
    "            print(\"have j gene\", self.j_gene)\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(\n",
    "                self.queried_dataframe.j_gene == self.j_gene\n",
    "            )\n",
    "\n",
    "        if self.regular_expression:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(\n",
    "                self.queried_dataframe.cdr3_aa.rlike(self.regular_expression)\n",
    "            )\n",
    "\n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        self.applied = True\n",
    "        return self.queried_dataframe\n",
    "\n",
    "    def get_normalized(self, dataframe, column=\"ez_donor\"):\n",
    "        q = self.apply(dataframe)\n",
    "        search_1 = q.groupby(\"ez_donor\").count()\n",
    "        search_2 = (\n",
    "            dataframe.groupby(\"ez_donor\")\n",
    "            .count()\n",
    "            .withColumnRenamed(\"count\", \"total_count\")\n",
    "        )\n",
    "        new_df = search_1.join(search_2, column).withColumn(\n",
    "            \"NormalizedCustomerValue\", (F.col(\"count\") / F.col(\"total_count\"))\n",
    "        )\n",
    "        return pd.DataFrame(new_df.collect(), columns=new_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCT64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "pct64_query = Query(\n",
    "    \"PCT64\",\n",
    "    v_fam=\"IGHV3\",\n",
    "    d_gene=\"IGHD3-3\",\n",
    "    j_gene=\"IGHJ6\",\n",
    "    length=25,\n",
    "    regex=r\"^......[YRKG][DSG]FWS..............$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_pct64 = pct64_query.get_normalized(df_spark)\n",
    "normal_query_df_pct64[\"class\"] = \"pct64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01 - CH04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "ch04_query = Query(\n",
    "    \"ch04\",\n",
    "    v_fam=\"IGHV3\",\n",
    "    j_gene=\"IGHJ2\",\n",
    "    length=26,\n",
    "    regex=r\"^..............Y[YQK]GSG.......$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_ch04 = ch04_query.get_normalized(df_spark)\n",
    "normal_query_df_ch04[\"class\"] = \"ch04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "pg9_query = Query(\n",
    "    \"pg9\",\n",
    "    v_fam=\"IGHV3\",\n",
    "    j_gene=\"IGHJ6\",\n",
    "    length=30,\n",
    "    regex=r\"^...............YDF............$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_pg9 = pg9_query.get_normalized(df_spark)\n",
    "normal_query_df_pg9[\"class\"] = \"pg9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGT145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "pgt145_33 = Query(\n",
    "    \"pgt33\",\n",
    "    v_fam=\"IGHV1\",\n",
    "    j_gene=\"IGHJ6\",\n",
    "    length=33,\n",
    "    regex=r\"^.............Y[GND][DEY].................$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_pgt145_33 = pgt145_33.get_normalized(df_spark)\n",
    "normal_query_df_pgt145_33[\"class\"] = \"pgt145_33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "pgt145_34 = Query(\n",
    "    \"pgt34\",\n",
    "    v_fam=\"IGHV1\",\n",
    "    j_gene=\"IGHJ6\",\n",
    "    length=34,\n",
    "    regex=r\"^.............Y[GND][DEY]..................$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_pgt145_34 = pgt145_34.get_normalized(df_spark)\n",
    "normal_query_df_pgt145_34[\"class\"] = \"pgt145_34\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pgt145_df = combine_dfs([normal_query_df_pgt145_34, normal_query_df_pgt145_33])\n",
    "pgt145_df[\"class\"] = \"pgt145\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAP256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "cap256_37 = Query(\n",
    "    \"cap256_37\",\n",
    "    v_fam=\"IGHV3\",\n",
    "    j_gene=\"IGHJ3\",\n",
    "    length=37,\n",
    "    regex=r\"^................YD[FIL]..................$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_cap256_37 = cap256_37.get_normalized(df_spark)\n",
    "normal_query_df_cap256_37[\"class\"] = \"cap256_37\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "cap256_38 = Query(\n",
    "    \"cap256_38\",\n",
    "    v_fam=\"IGHV3\",\n",
    "    j_gene=\"IGHJ3\",\n",
    "    length=38,\n",
    "    regex=r\"^.................YD[FIL]..................$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_cap256_38 = cap256_38.get_normalized(df_spark)\n",
    "normal_query_df_cap256_38[\"class\"] = \"cap256_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make query class\n",
    "cap256_39 = Query(\n",
    "    \"cap256_39\",\n",
    "    v_fam=\"IGHV3\",\n",
    "    j_gene=\"IGHJ3\",\n",
    "    length=39,\n",
    "    regex=r\"^................YD[FIL]....................$\",\n",
    ")\n",
    "\n",
    "# get normalized counts that turn it into a pandas datafram\n",
    "normal_query_df_cap256_39 = cap256_39.get_normalized(df_spark)\n",
    "normal_query_df_cap256_39[\"class\"] = \"cap256_39\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap256_df = combine_dfs([normal_query_df_cap256_37, normal_query_df_cap256_38])\n",
    "cap256_df[\"class\"] = \"cap256\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T04:22:28.861161Z",
     "iopub.status.busy": "2022-08-24T04:22:28.860928Z",
     "iopub.status.idle": "2022-08-24T04:22:28.916049Z",
     "shell.execute_reply": "2022-08-24T04:22:28.915332Z",
     "shell.execute_reply.started": "2022-08-24T04:22:28.861137Z"
    }
   },
   "source": [
    "# Combine all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat(\n",
    "    [\n",
    "        normal_query_df_pct64,\n",
    "        normal_query_df_ch04,\n",
    "        normal_query_df_pg9,\n",
    "        cap256_df.reset_index(),\n",
    "        pgt145_df.reset_index(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = final_df.rename({\"NormalizedCustomerValue\": \"normal\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pointplot(data=final_df, x=\"class\", y=\"normal\", hue=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
